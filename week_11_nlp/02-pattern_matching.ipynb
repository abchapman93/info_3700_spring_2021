{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import medspacy\n",
    "\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Overview\n",
    "As we saw in the last notebook, spaCy doesn't work great for clinical text out of the box. We're interested in extracting different types of information from clinical text than news or Wikipedia articles. Clinical text is also very different from general domain language. \n",
    "- **It is very messy**, with semi-structured formatting from EHR\n",
    "- Clinical documents include **many abbreviations**, some of which are ambiguous\n",
    "- There are **specific tasks** needed in clinical NLP, such as **detecting negation or uncertainty** for concepts in the text\n",
    "\n",
    "One of the most powerful components of spaCy is that is **very customizable**. In addition to working with the default models provided in the core library, you can create your own [custom components](https://spacy.io/usage/processing-pipelines#custom-components) or add your own [extension attributes](https://spacy.io/usage/processing-pipelines#custom-components-attributes). Developers and researchers can then publish their spaCy extensions to the open-source community. Some examples of these openly available libraries are:\n",
    "\n",
    "- [scispacy](https://allenai.github.io/scispacy/): Includes models trained on biomedical literature\n",
    "- [medCAT](https://github.com/CogStack/MedCAT): Models trained for medical concept extraction\n",
    "\n",
    "In the next two notebooks, we'll use [medspacy](https://github.com/medspacy/medspacy), a newly released package for performing clinical NLP tasks in spaCy. \n",
    "\n",
    "# medspacy\n",
    "<img alt=\"MedSpaCy logo\" src=\"https://github.com/medspacy/medspacy/raw/master/images/medspacy_logo.png\">\n",
    "\n",
    "\n",
    "[Medspacy](https://github.com/medspacy/medspacy) is an open-source package maintained by NLP developers at the University of Utah and the US Department of Veterans Affairs. The goal of medSpaCy is to provide flexible, easy-to-use spaCy components for common clinical NLP tasks, such as:\n",
    "\n",
    "- Concept extraction\n",
    "- Negation detection\n",
    "- Document section splitting\n",
    "\n",
    "One of the early uses of medSpaCy includes a [biosurveillance system for identifying positive cases of COVID-19](https://openreview.net/forum?id=ZQ_HvBxcdCv).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url=\"https://github.com/medspacy/medspacy/blob/master/images/medspacy_logo.png?raw=true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started with medspaCy\n",
    "To get started with medspaCy, we'll load a model using the `medspacy.load()` function, similar to spaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = medspacy.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the pipeline components that are loaded. Note that these are different than the components loaded with when we called `spacy.load(\"en_core_web_sm\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a summary of what each of these components does in our pipeline:\n",
    "- `sentencizer`: Splits a clinical document up into sentences\n",
    "- `target_matcher`: Rule-based extraction for identifying entities\n",
    "- `context`: Asserts attributes such as negation and family history for clinical concepts\n",
    "\n",
    "We'll look at the context component in a future notebook, but we'll start with `target_matcher`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable context for now\n",
    "_ = nlp.remove_pipe(\"context\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Concept extraction\n",
    "The first step we'll take is to define the **target concepts** we're interested in. In the previous notebook, spaCy extracted concepts like **\"PERSON\"** and **\"ORG\"**. In this notebook, we'll extract the following labels:\n",
    "- **\"PROBLEM\"**\n",
    "- **\"TREATMENT\"**\n",
    "- **\"TEST\"**\n",
    "\n",
    "Look at the text below. What examples of these clinical concepts can you find in the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"76 year old man with hypotension, CKD Stage 3, previously ckd stage two, status post RIJ line placement and Swan.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by building a **rule-based system**. In rule-based NLP, we define patterns to match concepts in text. SpaCy offers many [rule-based methods](https://spacy.io/usage/rule-based-matching). MedSpaCy uses a pipeline component called `TargetMatcher` and rules defined by a class called `TargetRule`. Extracted concepts will be stored as `Span` objects in `doc.ents`.\n",
    "\n",
    "We can access the target matcher through the `get_pipe()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import class for defining rules\n",
    "from medspacy.ner import TargetRule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_matcher = nlp.get_pipe(\"target_matcher\")\n",
    "target_matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target rules require two positional arguments:\n",
    "- `literal`: A span of text to match in the text (case insensitive)\n",
    "- `category`: The label to assign to extracted concepts\n",
    "\n",
    "Let's define rules to extract a few of the relevant clinical concepts in the texts above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_rules = [\n",
    "    TargetRule(\"hypotension\", \"CONDITION\"),\n",
    "    TargetRule(\"CKD Stage 3\", \"CONDITION\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then add these rules to our target matcher:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_matcher.add(target_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting matches\n",
    "Now, let's process our example texts and see what our model extracts. We can use medspaCy's `visualize_ent` to display our docs:\n",
    "\n",
    "### TODO\n",
    "Create a new `doc` by calling `nlp()` on the `text` variable. Then print out the entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we've now extracted the two patterns we defined in our **ruler**. Let's visualize this using `medspacy.visualization.visualize_ent()`, which offers extended visualizations for spaCy output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medspacy.visualization import visualize_ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_ent(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced pattern matching\n",
    "We could pass in simple strings to our `ruler` to extract exact matches. However, there may be lots of small variations in the text we want to extract, and it will grow cumbersome to type out every single possible string. Instead, we'll do some more advanced matching by using **token attribute matching**.\n",
    "\n",
    "SpaCy allows us to write patterns based on not only the exact text, but other linguistic attributes such as **part-of-speech tag**, **numerical properties**, **regular expressions**, and much more. \n",
    "\n",
    "## Example: Chronic Kidney Disease\n",
    "In the above text, we extracted two entities, including **\"CKD Stage 3\"**. However, there's a very similar span of text we want to extract: **\"ckd stage two\"**. We could write a new pattern to match this, but we would also want to match **\"CKD Stage 2\"**, **\"ckd Stage 4\"**, **\"CKD Stage 5\"**, etc. Instead of trying to think of the near-infinite number of variations, let's write one pattern which will match all of these clinical problems.\n",
    "\n",
    "An advanced pattern in spaCy is a Python **list**. Each element in that list is a **dictionary** representing each of the **tokens** (individual words) in a span of text. The **keys** of the dictionary represent the token attributes to look at and the **values** represent the values which should trigger a match:\n",
    "\n",
    "---\n",
    "```python\n",
    "[\n",
    "    {\"ATTRIBUTE\": value}, # First token\n",
    "    {\"ATTRIBUTE\": value}, # Second token\n",
    "    {\"ATTRIBUTE\": value} # Third token\n",
    "]\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now write a pattern which will match both **\"CKD Stage 3\"** and **\"ckd stage two\"**. What attributes are similar between these two spans of text? What is a general pattern that you could match?\n",
    "\n",
    "Both spans of text start out with the text **\"CKD\"**, although one is upper-case and one is lower-case. To match either, we will match on the **\"LOWER\"** attribute of the token:\n",
    "\n",
    "```python\n",
    "{\"LOWER\": \"ckd\"}\n",
    "```\n",
    "\n",
    "The second token is **\"Stage\"**, but again there's a difference in case. So let's use the **\"LOWER\"** attribute again:\n",
    "\n",
    "```python\n",
    "{\"LOWER\": \"stage\"}\n",
    "```\n",
    "\n",
    "Finally, the last token is a number. In this text there are **\"3\"** and **\"two\"**, but there could potentially be any number **1-5**. So let's just match any number. SpaCy can also recognize that the word **\"two\"** is a number by using the **\"LIKE_NUM\"** attribute, which is a boolean:\n",
    "\n",
    "```python\n",
    "{\"LIKE_NUM\": True}\n",
    "```\n",
    "\n",
    "When we put it all together, here is our pattern.\n",
    "\n",
    "### TODO\n",
    "Add the three dictionaries shown above in the **\"pattern\"** slot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckd_rule = TargetRule(\"CKD Stage X\", __,\n",
    "                     pattern=[\n",
    "                        {\"LOWER\": __}, # Token 1\n",
    "                        {__: \"stage\"}, # Token 2\n",
    "                        {\"LIKE_NUM\": True} # Token 3\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_matcher.add([ckd_rule])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_ent(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked! Our pattern will also match other variations of chronic kidney disease. Feel free to try it out yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treatment entities\n",
    "We've now extracted all of the **\"PROBLEM\"** entities from our text. The other class we're interested in now is **\"TREATMENT\"**, which could include medication, procedures, or therapies. In our text, the two treatments are **\"RIJ line placement\"** and **\"Swan\"**. \n",
    "\n",
    "### TODO\n",
    "Add two new patterns to match these treatments. You could either match on exact strings or more complex attributes (like lower-casing) as seen in the examples above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rules = [\n",
    "    TargetRule(__, __),\n",
    "    ____\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_matcher.add(new_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check which ents are extracted. Did you get all of the PROBLEM and TREATMENT entities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_ent(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Write your own rules\n",
    "\n",
    "Add rules to `target_matcher` that will extract the following concepts from these texts:\n",
    "- \"PROBLEM\"\n",
    "- \"TREATMENT\"\n",
    "\n",
    "First, identify all of the **problems** and **treatments** in the texts below. Then write rules and add them to `target_matcher` to extract them from the text. You can do either simple string matching (where the **\"pattern\"** value is a string and will match a string exactly) or more complex patterns (where the **\"pattern\"** value is a list of dicts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"87-year-old man with htn and end-stage renal disease.\",\n",
    "    \"His wife recently died from end stage renal disease.\",\n",
    "    \"The patient was started on abx for his infection\",\n",
    "    \"There is continued mild-to-moderate congestive heart failure. \",\n",
    "    \"The patient is s/p median sternotomy and right thoracotomy.\",\n",
    "    \"The pt presents for ckd stage 4\",\n",
    "    \"He previously had CKD stage 3.\",\n",
    "    \"The patient presented to the emergency room with cough and fever, concern for infections.\",\n",
    "    \"Patient prescribed coumadin for her atrial fibrillation\",\n",
    "    \"Patient prescribed coumadin for her AF\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = [\n",
    "    TargetRule(__, __),\n",
    "    # Add additional target rules here\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_matcher.add(rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll process all of our texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = list(nlp.pipe(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll need to go through the results and see if all the concepts we specified are being extracted. SpaCy has great visualization methods and medspaCy extends some of these for inspecting the results of a model.\n",
    "\n",
    "In the cell below, we'll use a Jupyter Widget to interactively scroll through the docs and visualize the output. Scroll through the docs using either the slider or the **\"Previous\"/\"Next\"** buttons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medspacy.visualization import MedspaCyVisualizerWidget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = MedspaCyVisualizerWidget(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "If the output above won't display, you may need to do some extra configuration to get widgets to show up in notebooks. First, try running these commands in your terminal:\n",
    "\n",
    "```bash\n",
    "pip install ipywidgets\n",
    "jupyter nbextension enable --py widgetsnbextension\n",
    "```\n",
    "\n",
    "Then restart your kernel and try again.\n",
    "\n",
    "If that doesn't work, you can manually change the index and visualize each doc one at at ime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = 0 # Change this to go through the docs\n",
    "# visualize_ent(docs[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "Rule-based systems can be very effective at extracting specific, targeted information from text. But they have disadvantages, such as that it is extremely manual effort to develop a comprehensive set of rules to extract concepts. \n",
    "\n",
    "In the next notebook we'll see how a **statistical model** can be used to extract information without writing specific rules.\n",
    "\n",
    "[03-statistical-nlp.ipynb](03-statistical-nlp.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
