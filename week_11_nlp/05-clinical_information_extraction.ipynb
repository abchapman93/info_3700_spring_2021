{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import medspacy\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medspacy.visualization import visualize_dep, visualize_ent, MedspaCyVisualizerWidget\n",
    "from medspacy.context import ConTextItem\n",
    "from medspacy.ner import TargetRule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework: Clinical Information Extraction\n",
    "Over the last two weeks, you've been introduced to a number of tools for extracting information from clinical text:\n",
    "- A rule-based matcher using the `TargetMatcher` class\n",
    "- A pre-trained statistical `NER` model for extracting **\"PROBLEM\"**, **\"TREATMENT\"**, and **\"TEST\"** entities\n",
    "- `ConTextComponent` for extracting contextual information such as negation, uncertainty, and family history\n",
    "\n",
    "For your homework assignment, we'll put it all together, improve our model, and deploy it on MIMIC data. Here is an outline of this assignment:\n",
    "\n",
    "- Build an medspaCy model which includes the `TargetMatcher`, statistical `NER`, and `ConTextComponent`\n",
    "- Load a sample of discharge summaries from MIMIC\n",
    "- Review the output of your NLP model on a small number of datasets and make imnprovements by adding patterns or ConTextItems\n",
    "- Deploy your NLP model on the entire dataset and convert it to structured data\n",
    "- Analyze the classes and spans of text extracted by your model\n",
    "\n",
    "As usual, let me know on Slack or Canvas if you have any questions or issues. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Build your model\n",
    "We'll create a new model by loading the various pieces which we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "Load a clinical `nlp` model using spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = medspacy.load(\"en_info_3700_i2b2_2012\", \n",
    "                    enable=[\"sentencizer\", \"target_matcher\", \"context\", \"sectionizer\"]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the two components that we will customize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_matcher = nlp.get_pipe(\"target_matcher\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = nlp.get_pipe(\"context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Get Discharge Summaries MIMIC Data\n",
    "A **discharge summary** is written at the end of a patient's stay in the hospital. It typically contains a summary of the patient, the diagnoses for which they were admitted, and the treatment that they received during their stay. The rich content of these documents makes them an excellent candidate for processing with NLP.\n",
    "\n",
    "Clinical documents are stored in MIMIC in the table `noteevents`. We will query a number of notes from this table and limit them to discharge summaries through the **\"category\"** column. We'll just look at 100 notes for now, but if you'd like to increase the number later to get a larger sample size you can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pymysql\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to your username\n",
    "username = \"uvu10919523\"\n",
    "\n",
    "conn = pymysql.connect(host=\"35.233.174.193\",port=3306,\n",
    "                       user=username,passwd=getpass.getpass(\"Enter password for MIMIC2 database\"),\n",
    "                       db='mimic2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT subject_id, text\n",
    "FROM noteevents\n",
    "WHERE category = 'DISCHARGE_SUMMARY'\n",
    "LIMIT 100\n",
    "\"\"\"\n",
    "df = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Process some texts and review the output\n",
    "Next, we'll process the discharge summaries and review what our system extracts. Processing full notes is a computationally expensive process, so we'll start by looking at just a few texts before processing the entire batch later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "texts = df[\"text\"].iloc[:5] # Small sample to start with\n",
    "docs = list(nlp.pipe(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medspacy.visualization import visualize_ent, visualize_dep\n",
    "from medspacy.visualization import MedspaCyVisualizerWidget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = MedspaCyVisualizerWidget(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = 0\n",
    "# visualize_ent(docs[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Improve your model\n",
    "As we've seen, our default model is not going to be perfect. If you'd like to spend some time improving your model, go through a few docs above and find mistakes. Then fix them using the methods we saw in previous notebooks.\n",
    "\n",
    "- **False negatives**: Missing a target entity. This will happen when you see a clinical problem, treatment or test in the text that is not highlighted. You can fix this by **adding patterns** to the `ruler`\n",
    "- **False positives**: Spans of text which are highlighted but should not be. These are harder to fix. You could write rules to remove an entity from `doc.ents`, but this is a little tricky and difficult to generalize\n",
    "- **Missing modifiers**: ConText modifiers, such as **\"NEGATED_EXISTENCE\"** will be highlighted in the text as well. If you see one that is missing, add it to ConText by creating a new `ConTextItem`. You can also visualize what targets the modifiers are applied to by using the `visualize_dep` function.\n",
    "    - **A note about `visualize_dep`**: This function works best on a *single* sentence rather than an entire doc. So instead of calling `visualize_dep(doc)`, manually add some text, process it with the nlp, and then view the output by calling:  `visualize_dep(nlp(\"...\"))`\n",
    "    \n",
    "Edit the cells below to add `TargetRules` and `ConTextItems` to fix mistakes you find in the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medspacy.ner import TargetRule\n",
    "from medspacy.context import ConTextItem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_matcher = nlp.get_pipe(\"target_matcher\")\n",
    "\n",
    "target_rules = [\n",
    "    # TargetRule(...),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = nlp.get_pipe(\"context\")\n",
    "\n",
    "context_rules = [\n",
    "    # ConTextItem(...)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've added new rules, go back to the cells at the beginning of this section, reprocess your docs, and reload your visualizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now go back, reprocess the doc, and see if your changes worked!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Deploy your model and convert text to structured data\n",
    "Now that you've fine-tuned and improved your model, we're ready to run it on the entire dataset and analyze it! In this step, we'll show how you can use NLP to convert text to **structured** data, which you can then analyze in the same way that we previously analyzed structured EHR data like **labs** and **vitals**. We'll now extract all of the entities from our docs and write them to a sqlite database.\n",
    "\n",
    "The function below will take your DataFrame, process all of the texts with your NLP model, and write the results to a file called `\"nlp.db\"`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import write_nlp_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may take up to 2-3 minutes\n",
    "write_nlp_db(nlp, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now connect to this local database using `sqlite3` and treat it like any other structured data. All of our data was written to a table called `ents`. We'll first load all of the results as a pandas dataframe, and in the next section we'll write queries to answer specific questions to explore the NLP-extracted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "nlp_conn = sqlite3.connect(\"nlp.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"SELECT * FROM ents\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ents_df = pd.read_sql(query, nlp_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the DataFrame below. What does each row correspond to? What do the various columns mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ents_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ents_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ents_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Analysis\n",
    "Now, we can analyze our extracted dataset using SQL, pandas, and matplotlib, just like we did with MIMIC data in the past. As a reminder, you can run queries by passing them into `pd.read_sql` along with our connection object, which in this case is `nlp_conn`.\n",
    "\n",
    "The table you will be querying is called `ents`.\n",
    "\n",
    "Go through each of the sections below and analyze the data to answer the question. You can either write queries to directly get numbers (ie, `SELECT ... FROM ents`), or use the DataFrame we created above, `ents_df`, to just run the analyses in pandas.\n",
    "\n",
    "If you need a reminder of how to use SQL/pandas, you can refer to the notebooks in [../week_6_clinical_data](../week_6_clinical_data) and [../week_7_terminologies](../week_7_terminologies)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Label distribution\n",
    "- Find the counts of **problems**, **treatments**, and **tests** which were extracted from our corpus \n",
    "- Plot the count of entity labels in the dataset using a bar graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    ____\n",
    "    ,____\n",
    "FROM ents\n",
    "GROUP BY ____\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_sql(query, nlp_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.____(x=\"label_\", \"COUNT(1)\", data=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Treatment texts\n",
    "Let's see what treatments are being used in these patient visits.\n",
    "- Find the 10 most common `\"text\"` values for **\"TREATMENT\"** entities\n",
    "- Plot a horizontal bar graph of the texts and counts. (Horizontal because that will make the labels easier to read)\n",
    "- Do any of these \"treatments\" look like NLP mistakes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatments = pd.read_sql(query, nlp_conn) \n",
    "treatments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a horizontal barplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Problems relevant to a visit\n",
    "As we saw in the previous notebook, many of the conditions mentioned in a document were not actually experienced by a patient during the hospital stay. That is why we ran **context** to generate the attributes such as **is_negated**. Let's now look at all problems in the dataset which are **relevant** to the dataset, meaning that all of the context attributes are `False` (ie., the problem is **not** historical, **not** negated, etc.)\n",
    "\n",
    "- Write a query which gets all **\"PROBLEM\"** entities from the database where all of the following columns are **0**:\n",
    "    - `is_negated`\n",
    "    - `is_historical`\n",
    "    - `is_uncertain`\n",
    "    - `is_family`\n",
    "    - `is_hypothetical`\n",
    "- Group them by **\"text\"** and find the 10 most common **\"text\"** spans\n",
    "- Plot a horizontal bar plot showing the counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relv_problems = pd.read_sql(query, nlp_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relv_problems.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a horizontal bar plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Patient family history\n",
    "In addition to **excluding** conditions which are not experienced by a patient, context can also help us target conditions which occurred in a patient's family history. While these conditions may not directly affect a patient, they are important to a patient's health because they might suffer from a heightened risk for this condition or other complications.\n",
    "\n",
    "In medspaCy, we can detect this by using the `is_family` attribute, or by seeing that an entity occurred in the `family_history` section of a note, which is shown by the `section_category` attribute. \n",
    "\n",
    "Let's now find patients with family history of cancer and see what types of cancer they have.\n",
    "\n",
    "### TODO\n",
    "- Write a query to get rows where:\n",
    "    - `label_` is **\"PROBLEM\"**\n",
    "    - `is_family` = **1** **OR** section_category = 'family_history'\n",
    "- Find the 10 most common text spans and plot them with a horizontal bar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fh = pd.read_sql(query, nlp_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fh.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a horizontal bar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
