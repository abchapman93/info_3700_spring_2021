{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "As we've seen, most of the machine learning process doesn't involve machine learning. A lot of our effort goes towards wrangling, visualizing, and cleaning up the data. But now we have our dataset in the format we need it and we're finally ready to do some prediction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by loading in the data which we saved in our last notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"diabetes_data.pkl\", \"rb\") as f:\n",
    "    (X_train, X_test, y_train, y_test) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Modeling the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a model\n",
    "To train a machine learning model, we **fit** it to the data. In scikit-learn, every model has the same `fit(X, y)` method.\n",
    "\n",
    "Once we fit a model, we can then use it to **predict** on new data.\n",
    "\n",
    "This **fit -> predict -> evaluate** pattern is one of the core components of machine learning. You should always fit using the training data and then predict and evaluate on the testing data.\n",
    "\n",
    "We'll start by training a [**Random Forest**](https://en.wikipedia.org/wiki/Random_forest) model. This tends to be a high-performing model on many datasets, so it's a good starting point. We won't go into too much detail with this model, but an image below shows the general process. A random forest consists of many decision trees (another type of ML model) working together to make a prediction. [This Medium article](https://medium.com/@williamkoehrsen/random-forest-simple-explanation-377895a60d2d) gives a high-level overview of this model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"./random_forest.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll import the model class from scikit-learn. Then we'll instantiate it and save it to a variable called `clf` (short for **\"classifier\"**)\"\n",
    "\n",
    "### TODO\n",
    "Import `RandomForestClassifier` from `sklearn.ensemble`. Then instantiate it and save it to a variable called `clf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "____ = ____()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll **fit** our model to the data by calling `clf.fit()` and passing in our training data.\n",
    "\n",
    "### TODO\n",
    "Call the random forest's fit method and pass in `X_train` and `y_train` as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.____(____, ____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating a model\n",
    "Once we have trained a model, we need to know how well it does. To achieve this, we'll see if it can correctly predict the classes of the testing data. \n",
    "\n",
    "First, we'll **predict** on `X_test` using `clf.predict(X_test)`. This will give us an array of predictions for each row which we'll compare with the labels for these rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "Create a variable called `pred` which is the output of `clf.predict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "____ = clf.____(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compare the labels and predictions. Let's start by doing it visually. We'll print out the first 25 labels and predictions and see if we can spot any differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Truth:\", list(y_test[:25]))\n",
    "print(\"Pred: \", list(pred[:25]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "A classifier is correct when the predicted label in `pred` is the same as the actual label in `y`. What does it mean when `y_test` and `pred` are different in these situations?\n",
    "- 1 is predicted but 0 is actual\n",
    "- 0 is actual but 1 is predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to have some numeric score to tell us how the dataset is doing as a whole. The simplest metric is **accuracy**, which tells us what proportion of times our model makes the correct prediction.  We can compute this by using the `accuracy_score` method from scikit-learn and passing in the labels and predictions.\n",
    "\n",
    "### TODO\n",
    "Compute the accuracy score by passing in `y_test` and `pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "____(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing an algorithm\n",
    "There are many different types of algorithms that can be used for machine learning classification. Each one works a little differently and some work better for a specific problem. We started by using a single model, but in practice you want to try out a a few different models and train each of them, then analyze and compare the results.\n",
    "\n",
    "We won't go into the details about each classifier, but we'll try out each of these 6 classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll save each of these models in a dictionary with the keys as an abbreviated name so we can keep track of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\"LR\": LogisticRegression(),\n",
    "          \"NB\": GaussianNB(),\n",
    "          \"KNN\": KNeighborsClassifier(),\n",
    "          \"DT\": DecisionTreeClassifier(),\n",
    "          \"RFC\": RandomForestClassifier(),\n",
    "          \"SVM\": SVC()\n",
    "         }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll loop through each of the models, fit them to our training data, predict on our testing data, and calculate the accuracy. We'll then save the name and accuracy score so we can analyze performance later.\n",
    "\n",
    "Since we're repeating steps for each model, it makes sense to turn these steps into a **function**.\n",
    "\n",
    "### TODO\n",
    "Finish defining the function below. Give it an informative name and two parameters: `clf` and `name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ____(____, ____):\n",
    "    print(\"Training {}\".format(name))\n",
    "    clf.____(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    pred = clf.____(X_test)\n",
    "    # Measure the accuracy\n",
    "    accuracy = accuracy_score(____, ____)\n",
    "    print(\"Accuracy: {}\".format(accuracy))\n",
    "    print()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's call our function on each of the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_scores = []\n",
    "model_names_scores = []\n",
    "for name, clf in models.items():\n",
    "    accuracy = train_model(clf, name)\n",
    "    \n",
    "    # Append to this list to analyze later\n",
    "    model_names_scores.append((name, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation and Analysis\n",
    "Let's see how our classifiers did on our test set. Let's start by sorting the scores by accuracy and plotting their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_name_scores = sorted(model_names_scores, key=lambda x:x[1], reverse=True)\n",
    "sorted_names, sorted_scores = zip(*sorted_name_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot = range(len(sorted_names))\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.bar(x_plot, sorted_scores)\n",
    "ax.set_ylim(0.5, 1.0)\n",
    "\n",
    "ax.set_xticks(x_plot)\n",
    "_ = ax.set_xticklabels(sorted_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closer analysis\n",
    "Now, let's take the best-performing model and look at more details. While training, we only looked at the **accuracy** of the classifier. But an accuracy score is sometimes insufficient. \n",
    "\n",
    "One thing to consider is **false positives** vs. **false negatives**. **False positives** occur when we incorrectly identify positive cases. In our scenario, this would mean saying a patient has a diabetes when they don't. **False negatives** are the reverse: when we fail to identify a positive case. For example, saying a patient doesn't have diabetes when they do.\n",
    "\n",
    "**Discussion**\n",
    "- Can you think of some reasons why accuracy might be insufficient?\n",
    "- Are false positives or false negatives worse?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To supplement this, we'll look at three more metrics: **precision**, **recall**, and **f1-score**. Here is a blog post that explains the difference between these metrics: https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9\n",
    "\n",
    "In summary, here's what each metric tells us:\n",
    "- **Precision**: If our classifier says a patient has diabetes, how likely is it that our classifier is correct? **A system which produces many *false positives* will have *lower* precision**\n",
    "- **Recall**: If our classifier is given a patient with diabetes, how likely is it that our classifier will correctly predict that? **A system which has many *false negatives* will have *lower* recall**\n",
    "- **F1**: This is a balanced average of the two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate these more detailed metrics with our highest-performing model. \n",
    "\n",
    "### TODO\n",
    "Look at the model accuracies you calculated above. Pick the highest-performing model, instantiate it, and retrain it. Then predict on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_clf = ____()\n",
    "best_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_clf.fit(X_train, y_train) # Retrain\n",
    "pred = best_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's calculate the precision, recall, and F1-score. Scikit-learn offers a useful function called `classification_report` which prints these values out for us. Since this is a binary task, we only really care about how well it performs on the positive class, so we'll restrict to looking at performance where the label is **1**.\n",
    "\n",
    "### TODO\n",
    "Call `classification_report` and compare `y_test` and `pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred, labels=[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "- Does our model's performance look as good when looking at these metrics as it did when we looked at accuracy? What accounts for the difference?\n",
    "- Does our model get more **false positives** or **false negatives**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving model performance\n",
    "There are many more steps you could take to improve the performance of your model. These steps include:\n",
    "- Scaling your data\n",
    "- Using cross-validation instead of train/test split\n",
    "- Hyperparamter tuning (see [this Medium article](https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74) for an example of this)\n",
    "\n",
    "This is out of scope for this module, but this Medium \n",
    "\n",
    "This [xkcd comic](https://xkcd.com/1838/) illustrates the general process of improving your system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"./xkcd_model.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Resources\n",
    "We just scraped the surface of machine learning today! There's so much to learn about this field, and the best way to learn is by doing hands-on projects and examples. If you are interested in learning more, here are some more resources:\n",
    "\n",
    "- [DataCamp Machine Learning Scientist with Python](https://www.datacamp.com/tracks/machine-learning-scientist-with-python): Here are 23 DataCamp courses which offer a fairly comprehensive overview of machine learning\n",
    "- [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/): This is the first textbook I used to learn about machine learning. It is available on O'Reilly, which you can access through the UVU library\n",
    "- [Andrew Ng's Machine Learning Coursera course](https://www.coursera.org/learn/machine-learning): A free and popular online class about machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "This week's homework will take another dataset, this one dealing with cardiovascular disease, and walk through the same steps as we've done in class today. Have these notebooks open as examples, and sometimes the best way to complete the homework exercises is to copy and paste from the in-class notebooks and change the variables.\n",
    "\n",
    "[./04-homework-master.ipynb](./04-homework-master.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
